# Meta-Monitoring and Continuous Improvement System

## Overview

This module implements an intelligent meta-monitoring system that continuously monitors, evaluates, and improves the FA AI System through automated analysis, validation, and feedback loops.

## Status

ğŸš§ **IN PROGRESS - Phase 1** ğŸš§

**Current Progress**:
- âœ… Planning complete (30-page implementation plan in `/docs/META_MONITORING_PLAN.md`)
- âœ… Database schema created (5 new tables)
- âœ… Database migration SQL ready
- â³ Agent implementations (pending)
- â³ API endpoints (pending)
- â³ UI components (pending)

## Architecture

```
Meta-Monitoring System
â”‚
â”œâ”€â”€ Agents
â”‚   â”œâ”€â”€ Monitoring Agent   - Continuous error detection (every 5 min)
â”‚   â”œâ”€â”€ Research Agent     - Root cause analysis + improvement proposals
â”‚   â”œâ”€â”€ Validation Agent   - Pre-deployment testing in sandbox
â”‚   â””â”€â”€ Evaluation Agent   - Daily quality scoring + ROI tracking
â”‚
â”œâ”€â”€ Infrastructure
â”‚   â”œâ”€â”€ Email Notifier     - Critical/hourly/daily alerts
â”‚   â”œâ”€â”€ Daily Scheduler    - APScheduler for automated tasks
â”‚   â””â”€â”€ API Layer          - FastAPI endpoints for admin control
â”‚
â””â”€â”€ UI
    â””â”€â”€ Admin Dashboard    - React UI for monitoring and approvals
```

## Database Schema

### New Tables

1. **meta_evaluation_runs** - Tracks daily evaluation runs
   - Metrics: fact accuracy, guardrail pass rate, SLA compliance
   - Comparisons vs previous day and baseline

2. **improvement_proposals** - Stores improvement suggestions from Research Agent
   - Root cause analysis
   - Estimated % improvement and effort
   - Review workflow (pending â†’ approved â†’ implemented)

3. **validation_results** - Pre-deployment test results
   - Test suite execution
   - Baseline vs test metrics comparison
   - Regression detection

4. **monitoring_alerts** - Active alerts for errors and anomalies
   - Severity levels (critical/high/medium/low)
   - LangSmith trace URLs for debugging
   - Email notification tracking

5. **improvement_impact** - Post-deployment ROI tracking
   - Before/after metrics
   - Statistical significance
   - ROI calculation

### Migration

Run the migration to create tables:
```bash
PGPASSWORD=dev_password psql -h localhost -U dev_user -d fa_ai_dev -f scripts/migrations/add_meta_monitoring_tables.sql
```

## Implementation Phases

### Phase 1: Foundation & Monitoring (10 days) - IN PROGRESS
- âœ… Database schema
- â³ Monitoring Agent
- â³ Evaluation Agent
- â³ Email notifications
- â³ Basic admin dashboard

### Phase 2: Intelligence & Analysis (12 days) - PENDING
- Research Agent
- Validation Agent
- Proposal management
- LangSmith prompt versioning
- Admin review UI

### Phase 3: Automation & Scheduling (8 days) - PENDING
- Daily scheduler
- Auto-approval logic
- Rollback mechanism
- Trend analysis
- Performance tracking

## Key Features

### 1. Continuous Monitoring
- Runs every 5 minutes
- Detects: error spikes, quality degradation, SLA violations, anomalies
- Triggers Research Agent when issues detected

### 2. Improvement Proposals
- Automated root cause analysis
- Specific fix recommendations
- Effort estimation (hours)
- % improvement prediction

### 3. Pre-Deployment Validation
- Tests changes in sandbox
- Golden test dataset (100 cases)
- Prevents regressions
- Statistical significance testing

### 4. Email Notifications
- **Immediate** (< 5 min): Critical errors, security issues
- **Hourly**: High-priority alerts
- **Daily** (9 AM): System health digest

### 5. Admin Dashboard
- Real-time metrics
- Active alerts
- Proposal review workflow
- Trend visualization
- LangSmith integration

## Directory Structure

```
src/meta_monitoring/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ monitoring_agent.py    # Error detection
â”‚   â”œâ”€â”€ research_agent.py      # Improvement proposals
â”‚   â”œâ”€â”€ validation_agent.py    # Pre-deployment testing
â”‚   â””â”€â”€ evaluation_agent.py    # Quality scoring
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ routes.py              # FastAPI endpoints
â”‚   â””â”€â”€ models.py              # Pydantic models
â”‚
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ daily_scheduler.py     # APScheduler jobs
â”‚
â”œâ”€â”€ notifications/
â”‚   â”œâ”€â”€ email_notifier.py      # Email service
â”‚   â””â”€â”€ templates/             # Email templates
â”‚
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ test_runner.py         # Validation test execution
â”‚   â””â”€â”€ rollback_manager.py    # Safe rollback
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ metrics_calculator.py  # Metric calculations
    â””â”€â”€ anomaly_detector.py    # Statistical anomaly detection
```

## LangSmith Integration

All meta-monitoring agents use prompts stored in LangSmith hub:

- `meta_error_analyzer` - Analyzes error patterns
- `meta_improvement_proposer` - Proposes improvements
- `meta_root_cause_analyzer` - Identifies root causes
- `meta_validation_evaluator` - Evaluates validation results
- `meta_anomaly_detector` - Detects statistical anomalies
- `meta_impact_calculator` - Calculates improvement impact

All operations are traced to **separate LangSmith project**:
- Project: `fa-ai-meta-monitoring` (to avoid circular dependencies)

## Safety Features

1. **Pre-Validation**: All changes tested before deployment
2. **Gradual Rollout**: 10% â†’ 50% â†’ 100% deployment
3. **Auto-Rollback**: Automatic revert if metrics degrade > 5%
4. **Human Approval**: Required for high-impact changes
5. **Exclusion List**: Meta-monitoring doesn't monitor itself

## Next Steps

1. Run database migration
2. Implement Monitoring Agent
3. Implement Evaluation Agent
4. Set up email notification system
5. Create API endpoints
6. Build admin dashboard
7. Test end-to-end workflow

## Resources

- **Full Plan**: `/docs/META_MONITORING_PLAN.md`
- **Database Models**: `/src/shared/models/meta_monitoring.py`
- **Migration SQL**: `/scripts/migrations/add_meta_monitoring_tables.sql`
- **LangSmith**: https://smith.langchain.com

---

**Version**: 1.0
**Status**: Phase 1 - In Progress
**Started**: 2025-11-08
**Estimated Completion**: 3-4 weeks (with 2 devs)
